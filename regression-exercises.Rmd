## Exercises 
1\. Create a dataset using the following code. 
```{r, eval=FALSE} 
n <- 100 
Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2) 
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) |> 
  data.frame() |> setNames(c("x", "y")) 
``` 
Use the __caret__ package to partition into a test and training set of equal size. Train a linear model and report the RMSE. Repeat this exercise 100 times and make a histogram of the RMSEs and report the average and standard deviation. Hint: adapt the code shown earlier like this: and put it inside a call to `replicate`.   
```{r, eval=FALSE} 
library(tidyverse)
library(dslabs)
library(caret)

n<-100
y <- dat$y 


MS <- replicate(100, {
  y <- dat$y
  test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE) 
train_set <- dat |> slice(-test_index) 
test_set <- dat |> slice(test_index) 
fit <- lm(y ~ x, data = train_set) 
y_hat <- fit$coef[1] + fit$coef[2]*test_set$x
RMSE<- sqrt(mean((y_hat - test_set$y)^2)) 

})

mean(MS)
sd(MS)
hist(MS)
```

2\. Now we will repeat the above but using larger datasets. Repeat exercise 1 but for datasets with `n <- c(100, 500, 1000, 5000, 10000)`. Save the average and standard deviation of RMSE from the 100 repetitions. Hint: use the `sapply` or `map` functions. 
```{r}
set.seed(1)
n <- c(100, 500, 1000, 5000, 10000)
regq2 <- sapply(n, function(n){
	Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
	dat <- MASS::mvrnorm(n, c(69, 69), Sigma) %>%
		data.frame() %>% setNames(c("x", "y"))
	rmse <- replicate(100, {
		test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
		train_set <- dat %>% slice(-test_index)
		test_set <- dat %>% slice(test_index)
		fit <- lm(y ~ x, data = train_set)
		y_hat <- predict(fit, newdata = test_set)
		sqrt(mean((y_hat-test_set$y)^2))
	})
	c(avg = mean(rmse), sd = sd(rmse))
})

regq2
```
3\. Describe what you observe with the RMSE as the size of the dataset becomes larger. 

a. On average, the RMSE does not change much as `n` gets larger, while the variability of RMSE does decrease. [x]
b. Because of the law of large numbers, the RMSE decreases: more data, more precise estimates. 
d. `n = 10000` is not sufficiently large. To see a decrease in RMSE, we need to make it larger. 
d. The RMSE is not a random variable. 
4\. Now repeat exercise 1, but this time make the correlation between `x` and `y` larger by changing `Sigma` like this: Repeat the exercise and note what happens to the RMSE now. 
```{r, eval=FALSE} 
n <- 100 
Sigma <- 9*matrix(c(1, 0.95, 0.95, 1), 2, 2) 
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) |> 
  data.frame() |> setNames(c("x", "y")) 
``` 

```{r}
MSq4 <- replicate(n, {
  test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE) 
train_set <- dat |> slice(-test_index) 
test_set <- dat |> slice(test_index) 
fit <- lm(y ~ x, data = train_set) 
y_hat <- fit$coef[1] + fit$coef[2]*test_set$x
RMSE<- sqrt(mean((y_hat - test_set$y)^2)) 

})
mean(MSq4)
sd(MSq4)

```
 
5\. Which of the following best explains why the RMSE in exercise 4 is so much lower than exercise 1. 
a. It is just luck. If we do it again, it will be larger. 
b. The Central Limit Theorem tells us the RMSE is normal. 
c. When we increase the correlation between `x` and `y`, `x` has more predictive power and thus provides a better estimate of `y`. This correlation has a much bigger effect on RMSE than `n`. Large `n` simply provide us more precise estimates of the linear model coefficients.[c] 
d. These are both examples of regression, so the RMSE has to be the same. 

6\.  Create a dataset using the following code: 
```{r, eval=FALSE} 
library(MASS)
n <- 1000 
#(didnt work weel)Sigma <- matrix(c(1, 3/4, 3/4, 3/4, 1, 0, 3/4, 0, 1), 3, 3) 
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.25, 0.75, 0.25, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) |> 
  data.frame() |> setNames(c("y", "x_1", "x_2")) 



``` 
Note that `y` is correlated with both `x_1` and `x_2`, but the two predictors are independent of each other. 
```{r, eval=FALSE} 
cor(dat) 
``` 
Use the __caret__ package to partition into a test and training set of equal size. Compare the RMSE when using just `x_1`, just `x_2`, and both `x_1` and `x_2`. Train a linear model and report the RMSE.  

```{r, eval=FALSE} 
n <- 1000


Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.95, 0.75, 0.95, 1.0), 3, 3) 
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) |> 
  data.frame() |> setNames(c("y", "x_1", "x_2")) 


test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE) 
train_set <- dat |> slice(-test_index) 
test_set <- dat |> slice(test_index)
```

```{r}
fit <- lm(y ~ x_1, data = train_set) 
y_hat <- fit$coef[1] + fit$coef[2]*test_set$x_1
RMSE<- sqrt(mean((y_hat - test_set$y)^2)) 
RMSE
``` 

```{r}
fit <- lm(y ~ x_2, data = train_set) 
y_hat <- fit$coef[1] + fit$coef[2]*test_set$x_2
RMSE<- sqrt(mean((y_hat - test_set$y)^2)) 
RMSE
```

```{r}
fit <- lm(y ~ x_1 + x_2, data = train_set) 
y_hat <- predict(fit, newdata = test_set)
RMSE<- sqrt(mean((y_hat - test_set$y)^2))
RMSE
```

7\.  Repeat exercise 6 but now create an example in which `x_1` and `x_2` are highly correlated: 
Use the __caret__ package to partition into a test and training set of equal size. Compare the RMSE when using just `x_1`, just `x_2`, and both `x_1` and `x_2` Train a linear model and report the RMSE. 
```{r, eval=FALSE} 
n <- 1000

Sigma <- matrix(c(1.0, 0.95, 0.95, 0.95, 1.0, 0.95, 0.95, 0.95, 1.0), 3, 3) 
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) |> 
  data.frame() |> setNames(c("y", "x_1", "x_2")) 

test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE) 
train_set <- dat |> slice(-test_index) 
test_set <- dat |> slice(test_index)
```

```{r}
fit <- lm(y ~ x_1, data = train_set) 
y_hat <- fit$coef[1] + fit$coef[2]*test_set$x_1
RMSE<- sqrt(mean((y_hat - test_set$y)^2)) 
RMSE
``` 

```{r}
fit <- lm(y ~ x_2, data = train_set) 
y_hat <- fit$coef[1] + fit$coef[2]*test_set$x_2
RMSE<- sqrt(mean((y_hat - test_set$y)^2)) 
RMSE
```

```{r}
fit <- lm(y ~ x_1 + x_2, data = train_set) 
y_hat <- predict(fit, newdata = test_set)
RMSE<- sqrt(mean((y_hat - test_set$y)^2))
RMSE
```

8\. Compare the results in 6 and 7 and choose the statement you agree with: 
a. Adding extra predictors can improve RMSE substantially, but not when they are highly correlated with another predictor.[x] 
b. Adding extra predictors improves predictions equally in both exercises. 
c. Adding extra predictors results in over fitting. 
d. Unless we include all predictors, we have no predicting power. 

## Exercises 
1\. Define the following dataset: 
```{r, eval = FALSE} 
make_data <- function(n = 1000, p = 0.5,  
                      mu_0 = 0, mu_1 = 2,  
                      sigma_0 = 1,  sigma_1 = 1){ 
  y <- rbinom(n, 1, p) 
  f_0 <- rnorm(n, mu_0, sigma_0) 
  f_1 <- rnorm(n, mu_1, sigma_1) 
  x <- ifelse(y == 1, f_1, f_0) 
  test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE) 
  list(train = data.frame(x = x, y = as.factor(y)) |>  
         slice(-test_index), 
       test = data.frame(x = x, y = as.factor(y)) |>  
         slice(test_index)) 
} 
dat <- make_data() 
``` 
Note that we have defined a variable `x` that is predictive of a binary outcome `y`. 
```{r, eval=FALSE} 
dat$train |> ggplot(aes(x, color = y)) + geom_density() 
``` 
Compare the accuracy of linear regression and logistic regression.
```{r}
fit <- lm(y ~ f_0 + f_1, data = train) 
y_hat_lm <- predict(fit, newdata = test)

y_hat_lm <- x |> factor()
confusionMatrix(y_hat_logit, test_set$sex)$overall[["Accuracy"]]
```

```{r}
fit_glm <- glm(y ~ x, data=dat$train, family = "binomial")
p_hat_glm <- predict(fit_glm, dat$test, type="response")
y_hat_glm <- factor(ifelse(p_hat_glm > 0.5, 1, 0))
confusionMatrix(y_hat_glm, dat$test$y)$overall["Accuracy"]
```
```{r}
fit_lm <- lm(as.numeric(y) ~ x, data=dat$train)
p_hat_lm <- predict(fit_lm, dat$test, type="response")
y_hat_lm <- factor(ifelse(p_hat_lm > 0.5, 1, 0))
confusionMatrix(y_hat_lm, dat$test$y)$overall["Accuracy"]
```
2\. Repeat the simulation from exercise 1 100 times and compare the average accuracy for each method and notice they give practically the same answer.
```{r}
n<-100


fit_glm.sim <- replicate(n, {
  fit_glm <- glm(y ~ x, data=dat$train, family = "binomial")
p_hat_glm <- predict(fit_glm, dat$test, type="response")
y_hat_glm <- factor(ifelse(p_hat_glm > 0.5, 1, 0))
confusionMatrix(y_hat_glm, dat$test$y)$overall["Accuracy"]

})

mean(fit_glm.sim)
sd(fit_glm.sim)
```
```{r}
n<-100

fit_lm.sim <- replicate(n, {
fit_lm <- lm(as.numeric(y) ~ x, data=dat$train)
p_hat_lm <- predict(fit_lm, dat$test, type="response")
y_hat_lm <- factor(ifelse(p_hat_lm > 0.5, 1, 0))
confusionMatrix(y_hat_lm, dat$test$y)$overall["Accuracy"]

})

mean(fit_lm.sim)
sd(fit_lm.sim)
```

3\. Generate 25 different datasets changing the difference between the two class: `delta <- seq(0, 3, len = 25)`. Plot accuracy versus `delta`. 

```{r}
delta <- seq(0, 3, len = 25)

res <- sapply(delta, function(d){
	dat <- make_data(mu_1 = d)
	fit_glm <- dat$train %>% glm(y ~ x, family = "binomial", data = .)
	y_hat_glm <- ifelse(predict(fit_glm, dat$test) > 0.5, 1, 0) |> factor(levels = c(0, 1))
	mean(y_hat_glm == dat$test$y)
})
qplot(delta, res)

```
 
