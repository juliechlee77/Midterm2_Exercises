---
output: html_document
editor_options: 
  chunk_output_type: console
---
## Exercises (31.2)
1\. Create a dataset using the following code. 
```{r}
#eval=FALSE} 
n <- 100 
Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2) 
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) |> 
  data.frame() |> setNames(c("x", "y")) 
``` 
Use the __caret__ package to partition into a test and training set of equal size. Train a linear model and report the RMSE. Repeat this exercise 100 times and make a histogram of the RMSEs and report the average and standard deviation. Hint: adapt the code shown earlier like this:  
```{r}
#eval=FALSE} 
#install.packages('DEoptimR') 
#install.packages("caret", dependencies = TRUE)
#install.packages("lifecycle")
library(ggplot2)
library(lattice)
library(caret)
library(tidyverse)
library(dslabs)
y <- dat$y 
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE) 
train_set <- dat |> slice(-test_index) 
test_set <- dat |> slice(test_index) 
fit <- lm(y ~ x, data = train_set) 
y_hat <- fit$coef[1] + fit$coef[2]*test_set$x 
sqrt(mean((y_hat - test_set$y)^2)) 
``` 
and put it inside a call to `replicate`. 

**ANSWER:**
```{r}
train_lm <- replicate(n, {
  test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE) 
  train_set <- dat |> slice(-test_index) 
  test_set <- dat |> slice(test_index) 
  fit <- lm(y ~ x, data = train_set) 
  y_hat <- fit$coef[1] + fit$coef[2]*test_set$x 
  RMSE <- sqrt(mean((y_hat - test_set$y)^2)) 
})

mean(train_lm)
sd(train_lm)
hist(train_lm)
```

2\. Now we will repeat the above but using larger datasets. Repeat exercise 1 but for datasets with `n <- c(100, 500, 1000, 5000, 10000)`. Save the average and standard deviation of RMSE from the 100 repetitions. Hint: use the `sapply` or `map` functions. 
**ANSWER:**
```{r}
n <- c(100, 500, 1000, 5000, 10000)

q2 <- function(n){
  # data set
  Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2) 
  dat <- MASS::mvrnorm(n = n, c(69, 69), Sigma) |> 
    data.frame() |> setNames(c("x", "y")) 
  
  # builds 100 linear models
  train_lm <- replicate(100, {
    test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE) 
    train_set <- dat |> slice(-test_index) 
    test_set <- dat |> slice(test_index) 
    fit <- lm(y ~ x, data = train_set) 
    y_hat <- fit$coef[1] + fit$coef[2]*test_set$x 
    RMSE <- sqrt(mean((y_hat - test_set$y)^2)) 
  })
  
  # find mean and standard deviation
  c(avg = mean(train_lm), sd = sd(train_lm))
}

# apply function to all n's
sapply(n, FUN = q2)
```

3\. Describe what you observe with the RMSE as the size of the dataset becomes larger. 
a. On average, the RMSE does not change much as `n` gets larger, while the variability of RMSE does decrease. 
b. Because of the law of large numbers, the RMSE decreases: more data, more precise estimates. 
c. `n = 10000` is not sufficiently large. To see a decrease in RMSE, we need to make it larger. 
d. The RMSE is not a random variable. 

**ANSWER: A**

4\. Now repeat exercise 1, but this time make the correlation between `x` and `y` larger by changing `Sigma` like this: 
```{r}
#eval=FALSE} 
n <- 100 
Sigma <- 9*matrix(c(1, 0.95, 0.95, 1), 2, 2) 
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) |> 
  data.frame() |> setNames(c("x", "y")) 
``` 
Repeat the exercise and note what happens to the RMSE now. 

**ANSWER:**
```{r}
train_lm <- replicate(n, {
  test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE) 
  train_set <- dat |> slice(-test_index) 
  test_set <- dat |> slice(test_index) 
  fit <- lm(y ~ x, data = train_set) 
  y_hat <- fit$coef[1] + fit$coef[2]*test_set$x 
  RMSE <- sqrt(mean((y_hat - test_set$y)^2)) 
})

mean(train_lm)
sd(train_lm)
# RMSE decreases
```

5\. Which of the following best explains why the RMSE in exercise 4 is so much lower than exercise 1. 
a. It is just luck. If we do it again, it will be larger. 
b. The Central Limit Theorem tells us the RMSE is normal. 
c. When we increase the correlation between `x` and `y`, `x` has more predictive power and thus provides a better estimate of `y`. This correlation has a much bigger effect on RMSE than `n`. Large `n` simply provide us more precise estimates of the linear model coefficients. 
d. These are both examples of regression, so the RMSE has to be the same. 

**ANSWER: C**

6\.  Create a dataset using the following code: 
```{r}
#eval=FALSE} 
n <- 1000 
Sigma <- matrix(c(1, 3/4, 3/4, 3/4, 1, 1/4, 3/4, 1/4, 1), 3, 3) 
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) |> 
  data.frame() |> setNames(c("y", "x_1", "x_2")) 
``` 
Note that `y` is correlated with both `x_1` and `x_2`, but the two predictors are independent of each other. 
```{r, eval=FALSE} 
cor(dat) 
``` 
Use the __caret__ package to partition into a test and training set of equal size. Compare the RMSE when using just `x_1`, just `x_2`, and both `x_1` and `x_2`. Train a linear model and report the RMSE.  

**ANSWER:**
```{r}
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE) 
train_set <- dat |> slice(-test_index) 
test_set <- dat |> slice(test_index) 

# x_1 only 
fit <- lm(y ~ x_1, data = train_set) 
y_hat <- fit$coef[1] + fit$coef[2]*test_set$x_1
sqrt(mean((y_hat - test_set$y)^2)) 

# x_2 only 
fit <- lm(y ~ x_2, data = train_set) 
y_hat <- fit$coef[1] + fit$coef[2]*test_set$x_2
sqrt(mean((y_hat - test_set$y)^2)) 

# x_1 and x_2 
fit <- lm(y ~ x_1 + x_2, data = train_set) 
#y_hat <- fit$coef[1] + fit$coef[2]*test_set$x_1 + fit$coef[3]*test_set$x_2
y_hat <- predict.lm(fit, test_set)
sqrt(mean((y_hat - test_set$y)^2)) 
# smallest RMSE for combined!!!
```

7\.  Repeat exercise 6 but now create an example in which `x_1` and `x_2` are highly correlated: 
```{r, eval=FALSE} 
n <- 1000 
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.95, 0.75, 0.95, 1.0), 3, 3) 
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) |> 
  data.frame() |> setNames(c("y", "x_1", "x_2")) 
``` 
Use the __caret__ package to partition into a test and training set of equal size. Compare the RMSE when using just `x_1`, just `x_2`, and both `x_1` and `x_2` Train a linear model and report the RMSE.  

**ANSWER:**
```{r}
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE) 
train_set <- dat |> slice(-test_index) 
test_set <- dat |> slice(test_index) 

# x_1 only 
fit <- lm(y ~ x_1, data = train_set) 
y_hat <- fit$coef[1] + fit$coef[2]*test_set$x_1
sqrt(mean((y_hat - test_set$y)^2)) 

# x_2 only 
fit <- lm(y ~ x_2, data = train_set) 
y_hat <- fit$coef[1] + fit$coef[2]*test_set$x_2
sqrt(mean((y_hat - test_set$y)^2)) 

# x_1 and x_2 
fit <- lm(y ~ x_1 + x_2, data = train_set) 
y_hat <- predict.lm(fit, test_set)
sqrt(mean((y_hat - test_set$y)^2)) 

# similar RMSE for all 3!!!
```


8\. Compare the results in 6 and 7 and choose the statement you agree with: 
a. Adding extra predictors can improve RMSE substantially, but not when they are highly correlated with another predictor. 
b. Adding extra predictors improves predictions equally in both exercises. 
c. Adding extra predictors results in over fitting. 
d. Unless we include all predictors, we have no predicting power. 

**ANSWER: A**

## Exercises (31.4)
1\. Define the following dataset: 
```{r}
#eval = FALSE} 
make_data <- function(n = 1000, p = 0.5,  
                      mu_0 = 0, mu_1 = 2,  
                      sigma_0 = 1,  sigma_1 = 1){ 
  y <- rbinom(n, 1, p) 
  f_0 <- rnorm(n, mu_0, sigma_0) 
  f_1 <- rnorm(n, mu_1, sigma_1) 
  x <- ifelse(y == 1, f_1, f_0) 
  test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE) 
  list(train = data.frame(x = x, y = as.factor(y)) |>  
         slice(-test_index), 
       test = data.frame(x = x, y = as.factor(y)) |>  
         slice(test_index)) 
} 
dat <- make_data() 
``` 
Note that we have defined a variable `x` that is predictive of a binary outcome `y`. 
```{r, eval=FALSE} 
dat$train |> ggplot(aes(x, color = y)) + geom_density() 
``` 
Compare the accuracy of linear regression and logistic regression.  

**ANSWER: logistic has higher accuracy**
```{r}
# linear -- this isn't right., makes no sense 
fit_lm <- lm(as.numeric(y) ~ x, data=dat$train)
p_hat_lm <- predict(fit_lm, dat$test, type="response")
y_hat_lm <- factor(ifelse(p_hat_lm > 0.5, 1, 0))
confusionMatrix(y_hat_lm, dat$test$y)$overall["Accuracy"]

# logistic
fit_glm <- glm(y ~ x, data=dat$train, family = "binomial")
y_hat_glm <- ifelse(predict(fit_glm, dat$test) > 0.5, 1, 0) |>
    factor(levels = c(0, 1))
confusionMatrix(y_hat_glm, dat$test$y)$overall["Accuracy"]
```

2\. Repeat the simulation from exercise 1 100 times and compare the average accuracy for each method and notice they give practically the same answer. 

**ANSWER:???????????????**
```{r}
logistic <- replicate(100, {
  fit_glm <- glm(y ~ x, data=dat$train, family = "binomial")
  p_hat_glm <- predict(fit_glm, dat$test, type="response")
  y_hat_glm <- ifelse(predict(fit_glm, dat$test) > 0.5, 1, 0) |>
    factor(levels = c(0, 1))  
  mean(y_hat_glm == dat$test$y)
})

mean(logistic)

linear <- replicate(100, {
  fit_lm <- lm(as.numeric(y) ~ x, data=dat$train)
  #p_hat_lm <- predict(fit_lm, dat$test, type="response")
  y_hat_lm <- predict(fit_lm, dat$x)
  mean(y_hat_lm == dat$test$y)
})
mean(linear)
```

3\. Generate 25 different datasets changing the difference between the two class: `delta <- seq(0, 3, len = 25)`. Plot accuracy versus `delta`. 

**ANSWER:**
```{r}
delta <- seq(0, 3, len = 25)

q3 <- sapply(delta, function(d){
  dat <- make_data(mu_1 = d)
  fit_glm <- glm(y ~ x, family = "binomial", data = dat$train)
  y_hat_glm <- ifelse(predict(fit_glm, dat$test) > 0.5, 1, 0) |>
    factor(levels = c(0, 1))
  mean(y_hat_glm == dat$test$y)
})
qplot(delta, q3)
```

 
